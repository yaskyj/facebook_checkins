{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 s, sys: 589 ms, total: 1.95 s\n",
      "Wall time: 4.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ml_metrics as metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as itertools\n",
    "from sklearn import cross_validation, ensemble, tree, \\\n",
    "    preprocessing, neighbors, naive_bayes, svm, cluster, linear_model, pipeline\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create ids and predictions lists...\n",
      "Lists created: 0.0 s\n",
      "Importing training and testing sets...\n",
      "Set imported: 38.623 s\n",
      "Binning data...\n",
      "Data binned: 1.963 s\n",
      "Running training loop...\n",
      "Bin 33,89\n",
      "Place IDs: 260\n",
      "Train length: 2352 Test length: 734\n",
      "[[4094839162 8325684763 9324235100]\n",
      " [4094839162 9324235100 8325684763]\n",
      " [9219620182 9594287947 5530332608]\n",
      " ..., \n",
      " [9219620182 7973182224 3308992460]\n",
      " [4094839162 8325684763 6462310224]\n",
      " [8325684763 2611982497 8286414795]]\n",
      "Analysis time: 2.575 s\n",
      "Bin 85,29\n",
      "Place IDs: 245\n",
      "Train length: 3812 Test length: 1273\n",
      "[[9380341391 7109098127 5994659302]\n",
      " [7647143004 8909014493 5155966178]\n",
      " [7109098127 3820116221 9011559746]\n",
      " ..., \n",
      " [9380341391 5994659302 7109098127]\n",
      " [7647143004 7795193871 8909014493]\n",
      " [9380341391 7109098127 5994659302]]\n",
      "Analysis time: 2.151 s\n",
      "Bin 68,20\n",
      "Place IDs: 231\n",
      "Train length: 3816 Test length: 801\n",
      "[[6086612539 9476803556 5806690964]\n",
      " [3252837854 3039007938 3473522304]\n",
      " [9476803556 5851410142 6086612539]\n",
      " ..., \n",
      " [5806690964 5851410142 3039007938]\n",
      " [3252837854 3473522304 7547799880]\n",
      " [5851410142 9476803556 5806690964]]\n",
      "Analysis time: 2.089 s\n",
      "Bin 35,96\n",
      "Place IDs: 259\n",
      "Train length: 2595 Test length: 511\n",
      "[[6728294162 4192941191 3959256445]\n",
      " [3263569929 6728294162 7594519646]\n",
      " [9238864336 2701868104 1801010249]\n",
      " ..., \n",
      " [9673415196 3959256445 6728294162]\n",
      " [8283471884 3959256445 8135488370]\n",
      " [8283471884 3967487477 8135488370]]\n",
      "Analysis time: 2.295 s\n",
      "Bin 69,62\n",
      "Place IDs: 279\n",
      "Train length: 3388 Test length: 1080\n",
      "[[7376973577 1920573324 3704630414]\n",
      " [8218974227 5328001338 9708699557]\n",
      " [3704630414 7376973577 6961149401]\n",
      " ..., \n",
      " [2669955780 9439678564 5800470682]\n",
      " [9708699557 6419523543 5328001338]\n",
      " [5328001338 8218974227 9708699557]]\n",
      "Analysis time: 2.269 s\n",
      "Bin 11,5\n",
      "Place IDs: 211\n",
      "Train length: 3041 Test length: 759\n",
      "[[8347703842 6376379532 4844599792]\n",
      " [4844599792 7951037972 3886043816]\n",
      " [4844599792 6376379532 5873863581]\n",
      " ..., \n",
      " [9723297030 5342781963 5927863550]\n",
      " [2938951386 2462266136 5342781963]\n",
      " [4844599792 6376379532 8347703842]]\n",
      "Analysis time: 2.451 s\n",
      "Bin 78,20\n",
      "Place IDs: 238\n",
      "Train length: 2900 Test length: 802\n",
      "[[2469180245 3646679179 6124682652]\n",
      " [6211537023 6124682652 5769711907]\n",
      " [4632542040 8935787116 6279891601]\n",
      " ..., \n",
      " [5629586366 1854420827 5769711907]\n",
      " [4967100065 2684971081 4632542040]\n",
      " [6124682652 5769711907 5629586366]]\n",
      "Analysis time: 2.323 s\n",
      "Bin 29,51\n",
      "Place IDs: 275\n",
      "Train length: 2000 Test length: 680\n",
      "[[6578259841 8034514927 1850129705]\n",
      " [6578259841 9093273274 2286356126]\n",
      " [6578259841 8034514927 1564183304]\n",
      " ..., \n",
      " [1850129705 8034514927 4652348473]\n",
      " [8034514927 6578259841 3790563379]\n",
      " [6578259841 9093273274 3790563379]]\n",
      "Analysis time: 2.212 s\n",
      "Bin 18,19\n",
      "Place IDs: 244\n",
      "Train length: 3154 Test length: 757\n",
      "[[7628144189 7727433665 5371103211]\n",
      " [5371103211 5832832330 5245912259]\n",
      " [5245912259 2690099194 5832936152]\n",
      " ..., \n",
      " [5245912259 2075216122 4773601656]\n",
      " [1752358384 8656934286 3932102096]\n",
      " [5077391476 7449189902 1752358384]]\n",
      "Analysis time: 2.527 s\n",
      "Bin 29,85\n",
      "Place IDs: 288\n",
      "Train length: 4289 Test length: 1309\n",
      "[[4104693813 5599126040 4829650890]\n",
      " [4829650890 7097062822 4104693813]\n",
      " [9608074554 8449461889 4104693813]\n",
      " ..., \n",
      " [3505901128 8926090596 7097062822]\n",
      " [3651360333 3425115549 2205480315]\n",
      " [3651360333 2205480315 4829650890]]\n",
      "Analysis time: 2.398 s\n",
      "Training loop completed: 1.06 m\n",
      "Id and predictions lengths...\n",
      "10 10\n",
      "Printed lengths: 0.0 s\n",
      "Flattening ids and predictions lists...\n",
      "Lists flattened: 0.009 s\n",
      "Checking list lengths again...\n",
      "8706 8706\n",
      "List lengths checked: 0.0 s\n",
      "Creating submission file...\n",
      "Submission file created: 0.085 s\n",
      "Exporting submission file...\n",
      "Submission file exported: 0.023 s\n",
      "Script End: 1.07 m\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "t0 = time()\n",
    "print \"Create ids and predictions lists...\"\n",
    "ids = []\n",
    "predictions = []\n",
    "print \"Lists created:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "t0 = time()\n",
    "print \"Importing training and testing sets...\"\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print \"Set imported:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "def horizontally_bin_data(data, NX, NY):\n",
    "    \"\"\"Add columns to data indicating X and Y bins.\n",
    "\n",
    "    Divides the grid into `NX` bins in X and `NY` bins in Y, and adds columns \n",
    "    to `data` containing the bin number in X and Y. \n",
    "    \"\"\"\n",
    "\n",
    "    NX = int(NX)\n",
    "    NY = int(NY)\n",
    "\n",
    "    assert((NX >= 5) and (NX <= 1000))\n",
    "    assert((NY >= 5) and (NY <= 1000))\n",
    "\n",
    "    x_bounds = (0., 10.)\n",
    "    y_bounds = (0., 10.)\n",
    "\n",
    "    delta_X = (x_bounds[1] - x_bounds[0]) / float(NX)\n",
    "    delta_Y = (y_bounds[1] - y_bounds[0]) / float(NY)\n",
    "\n",
    "    # very fast binning algorithm, just divide by delta and round down\n",
    "    xbins = np.floor((data.x.values - x_bounds[0])\n",
    "                     / delta_X).astype(np.int32)\n",
    "    ybins = np.floor((data.y.values - y_bounds[0])\n",
    "                     / delta_Y).astype(np.int32)\n",
    "\n",
    "    # some points fall on the upper/right edge of the domain\n",
    "    # tweak their index to bring them back in the box\n",
    "    xbins[xbins == NX] = NX-1\n",
    "    ybins[ybins == NY] = NY-1\n",
    "\n",
    "    xlabel = 'x_bin_{0:03d}'.format(NX)\n",
    "    ylabel = 'y_bin_{0:03d}'.format(NY)\n",
    "\n",
    "    data[xlabel] = xbins\n",
    "    data[ylabel] = ybins\n",
    "    return\n",
    "\n",
    "t0 = time()\n",
    "print \"Binning data...\"\n",
    "horizontally_bin_data(train, 100, 100)\n",
    "horizontally_bin_data(test, 100, 100)\n",
    "print \"Data binned:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "t0 = time()\n",
    "print \"Running training loop...\"\n",
    "estimator = 60\n",
    "#Choose this line for the whole dataset.\n",
    "rs = np.random.RandomState(34)\n",
    "bin_numbers = zip(rs.randint(0, 100, size=5), rs.randint(0, 100, size=5))\n",
    "\n",
    "#Choose this line for the whole dataset.\n",
    "# for i_bin_x, i_bin_y in itertools.product(xrange(50,51), xrange(50,60)):\n",
    "for i_bin_x, i_bin_y in bin_numbers:\n",
    "    t1 = time()\n",
    "    print(\"Bin {},{}\".format(i_bin_x, i_bin_y))\n",
    "\n",
    "    training_set = train[(train.x_bin_100 == i_bin_x) & (train.y_bin_100 == i_bin_y)]\n",
    "    testing_set = test[(test.x_bin_100 == i_bin_x) & (test.y_bin_100 == i_bin_y)]\n",
    "\n",
    "    print 'Place IDs:', len(np.unique(training_set['place_id']))\n",
    "    print 'Train length:', len(training_set), 'Test length:', len(testing_set)\n",
    "\n",
    "    minute = 2*np.pi*((training_set[\"time\"]//5)%288)/288\n",
    "    training_set['minute'] = minute\n",
    "    training_set['minute_sin'] = (np.sin(minute)+1).round(4)\n",
    "    training_set['minute_cos'] = (np.cos(minute)+1).round(4)\n",
    "    del minute\n",
    "    day = 2*np.pi*((training_set['time']//1440)%365)/365\n",
    "    training_set['day'] = day\n",
    "    training_set['day_of_year_sin'] = (np.sin(day)+1).round(4)\n",
    "    training_set['day_of_year_cos'] = (np.cos(day)+1).round(4)\n",
    "    del day\n",
    "    weekday = 2*np.pi*((training_set['time']//1440)%7)/7\n",
    "    training_set['weekday'] = weekday\n",
    "    training_set['weekday_sin'] = (np.sin(weekday)+1).round(4)\n",
    "    training_set['weekday_cos'] = (np.cos(weekday)+1).round(4)\n",
    "    del weekday\n",
    "    training_set['year'] = (((training_set['time'])//525600))\n",
    "    training_set.drop(['time'], axis=1, inplace=True)\n",
    "    training_set['month'] = ((training_set['weekday']//30)%12+1)*2.73\n",
    "    training_set['accuracy'] = np.log10(training_set['accuracy'])*14.4\n",
    "\n",
    "    training_set.loc[:,'x'] *= 465.0\n",
    "    training_set.loc[:,'y'] *= 975.0\n",
    "    training_set['squadd']= (training_set.x**2 + training_set.y**2)\n",
    "\n",
    "    \n",
    "    \n",
    "    minute = 2*np.pi*((testing_set[\"time\"]//5)%288)/288\n",
    "    testing_set['minute'] = minute\n",
    "    testing_set['minute_sin'] = (np.sin(minute)+1).round(4)\n",
    "    testing_set['minute_cos'] = (np.cos(minute)+1).round(4)\n",
    "    del minute\n",
    "    day = 2*np.pi*((testing_set['time']//1440)%365)/365\n",
    "    testing_set['day'] = day\n",
    "    testing_set['day_of_year_sin'] = (np.sin(day)+1).round(4)\n",
    "    testing_set['day_of_year_cos'] = (np.cos(day)+1).round(4)\n",
    "    del day\n",
    "    weekday = 2*np.pi*((testing_set['time']//1440)%7)/7\n",
    "    testing_set['weekday'] = weekday\n",
    "    testing_set['weekday_sin'] = (np.sin(weekday)+1).round(4)\n",
    "    testing_set['weekday_cos'] = (np.cos(weekday)+1).round(4)\n",
    "    del weekday\n",
    "    testing_set['year'] = (((testing_set['time'])//525600))\n",
    "    testing_set.drop(['time'], axis=1, inplace=True)\n",
    "    testing_set['month'] = ((testing_set['weekday']//30)%12+1)*2.73\n",
    "    testing_set['accuracy'] = np.log10(testing_set['accuracy'])*14.4\n",
    "\n",
    "    testing_set.loc[:,'x'] *= 465.0\n",
    "    testing_set.loc[:,'y'] *= 975.0\n",
    "    testing_set['squadd']= (testing_set.x**2 + testing_set.y**2)\n",
    "\n",
    "    features = [c for c in training_set.columns if c in ['year','month','dow','squadd','hour','time_kde','x_x', 'y_x','acc_norm']]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labels = le.fit_transform(training_set.place_id.values)\n",
    "\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(training_set.drop(['row_id', 'place_id', 'x_bin_100', 'y_bin_100',], axis=1).values, labels)\n",
    "    probs = forest.predict_proba(testing_set.drop(['row_id', 'x_bin_100', 'y_bin_100'], axis=1).values)\n",
    "\n",
    "    # probs.columns = np.unique(training_set['place_id'].sort_values().values)\n",
    "    preds = pd.DataFrame(le.inverse_transform(np.argsort(probs, axis=1)[:,::-1][:,:3]))\n",
    "\n",
    "    \n",
    "    ids.append(list(testing_set['row_id'].values))\n",
    "    predictions.append(preds.values)\n",
    "    print preds.values\n",
    "    print \"Analysis time:\",round(time()-t1,3),\"s\"\n",
    "print \"Training loop completed:\",round((time()-start_time)/60,2),\"m\"\n",
    "\n",
    "t0 = time()\n",
    "print \"Id and predictions lengths...\"\n",
    "print len(ids), len(predictions)\n",
    "print \"Printed lengths:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "t0 = time()\n",
    "print \"Flattening ids and predictions lists...\"\n",
    "ids = [val for sublist in ids for val in sublist]\n",
    "predictions = [val for sublist in predictions for val in sublist]\n",
    "print \"Lists flattened:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "t0 = time()\n",
    "print \"Checking list lengths again...\"\n",
    "print len(ids), len(predictions)\n",
    "print \"List lengths checked:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "t0 = time()\n",
    "print \"Creating submission file...\"\n",
    "submission = pd.DataFrame()\n",
    "submission['row_id'] = ids\n",
    "submission['place_id'] = [' '.join(str(x) for x in y) for y in predictions]\n",
    "submission.sort_values('row_id', inplace=True)\n",
    "print \"Submission file created:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "t0 = time()\n",
    "print \"Exporting submission file...\"\n",
    "submission.to_csv('final_submission.csv', index=False)\n",
    "print \"Submission file exported:\",round(time()-t0,3),\"s\"\n",
    "\n",
    "print \"Script End:\",round((time()-start_time)/60,2),\"m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 s, sys: 1.96 s, total: 23.8 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def horizontally_bin_data(data, NX, NY):\n",
    "    \"\"\"Add columns to data indicating X and Y bins.\n",
    "\n",
    "    Divides the grid into `NX` bins in X and `NY` bins in Y, and adds columns \n",
    "    to `data` containing the bin number in X and Y. \n",
    "    \"\"\"\n",
    "\n",
    "    NX = int(NX)\n",
    "    NY = int(NY)\n",
    "\n",
    "    assert((NX >= 5) and (NX <= 1000))\n",
    "    assert((NY >= 5) and (NY <= 1000))\n",
    "\n",
    "    x_bounds = (0., 10.)\n",
    "    y_bounds = (0., 10.)\n",
    "\n",
    "    delta_X = (x_bounds[1] - x_bounds[0]) / float(NX)\n",
    "    delta_Y = (y_bounds[1] - y_bounds[0]) / float(NY)\n",
    "\n",
    "    # very fast binning algorithm, just divide by delta and round down\n",
    "    xbins = np.floor((data.x.values - x_bounds[0])\n",
    "                     / delta_X).astype(np.int32)\n",
    "    ybins = np.floor((data.y.values - y_bounds[0])\n",
    "                     / delta_Y).astype(np.int32)\n",
    "\n",
    "    # some points fall on the upper/right edge of the domain\n",
    "    # tweak their index to bring them back in the box\n",
    "    xbins[xbins == NX] = NX-1\n",
    "    ybins[ybins == NY] = NY-1\n",
    "\n",
    "    xlabel = 'x_bin_{0:03d}'.format(NX)\n",
    "    ylabel = 'y_bin_{0:03d}'.format(NY)\n",
    "\n",
    "    data[xlabel] = xbins\n",
    "    data[ylabel] = ybins\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 703 ms, sys: 493 ms, total: 1.2 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "horizontally_bin_data(train, 100, 100)\n",
    "# horizontally_bin_data(test, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 33,89\n",
      "Place IDs: 260\n",
      "Estimators: 60 MapK: 0.468537414966\n",
      "Done in: 2.671 s\n",
      "Bin 85,29\n",
      "Place IDs: 245\n",
      "Estimators: 60 MapK: 0.673487233298\n",
      "Done in: 3.717 s\n",
      "Bin 68,20\n",
      "Place IDs: 231\n",
      "Estimators: 60 MapK: 0.591719077568\n",
      "Done in: 3.254 s\n",
      "Bin 35,96\n",
      "Place IDs: 259\n",
      "Estimators: 60 MapK: 0.442732408834\n",
      "Done in: 2.352 s\n",
      "Bin 69,62\n",
      "Place IDs: 279\n",
      "Estimators: 60 MapK: 0.462416371507\n",
      "Done in: 4.206 s\n",
      "Bin 11,5\n",
      "Place IDs: 211\n",
      "Estimators: 60 MapK: 0.649802890933\n",
      "Done in: 2.403 s\n",
      "Bin 78,20\n",
      "Place IDs: 238\n",
      "Estimators: 60 MapK: 0.466206896552\n",
      "Done in: 2.897 s\n",
      "Bin 29,51\n",
      "Place IDs: 275\n",
      "Estimators: 60 MapK: 0.628\n",
      "Done in: 2.268 s\n",
      "Bin 18,19\n",
      "Place IDs: 244\n",
      "Estimators: 60 MapK: 0.51436417406\n",
      "Done in: 3.285 s\n",
      "Bin 29,85\n",
      "Place IDs: 288\n",
      "Estimators: 60 MapK: 0.458372165269\n",
      "Done in: 5.022 s\n",
      "0.535563863299\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "estimator = 60\n",
    "rs = np.random.RandomState(34)\n",
    "bin_numbers = zip(rs.randint(0, 100, size=10), rs.randint(0, 100, size=10))\n",
    "# bin_numbers = rs.randint(0, 100, size=20)\n",
    "\n",
    "map3s = []\n",
    "fw = [0.6, 0.32935, 0.56515, 0.2670, 22, 52, 0.51785]\n",
    "#Choose this line for the whole dataset.\n",
    "# for i_bin_x, i_bin_y in itertools.product(xrange(50,51), xrange(50,60)):\n",
    "for i_bin_x, i_bin_y in bin_numbers:\n",
    "# for i_bin_y in bin_numbers:\n",
    "\n",
    "    t0 = time()\n",
    "    print(\"Bin {},{}\".format(i_bin_x, i_bin_y))\n",
    "\n",
    "    # choose the correct bin, sort values in time to better simulate\n",
    "    train_in_bin = train[(train.x_bin_100 == i_bin_x) & (train.y_bin_100 == i_bin_y)].sort_values('time')\n",
    "    #     train_in_bin = train[(train.y_bin_1000 == i_bin_y)].sort_values('time')\n",
    "\n",
    "    print 'Place IDs:', len(np.unique(train_in_bin['place_id']))\n",
    "\n",
    "    training_set = train_in_bin[:int(len(train_in_bin) - (len(train_in_bin)*.25))]\n",
    "    #     training_set = train_in_bin[int(len(train_in_bin)*.1):int(len(train_in_bin) - (len(train_in_bin)*.35))]\n",
    "\n",
    "    testing_set = train_in_bin[int(len(train_in_bin) - (len(train_in_bin)*.25)):]\n",
    "    \n",
    "\n",
    "    minute = 2*np.pi*((training_set[\"time\"]//5)%288)/288\n",
    "    training_set['minute'] = minute\n",
    "    training_set['minute_sin'] = (np.sin(minute)+1).round(4)\n",
    "    training_set['minute_cos'] = (np.cos(minute)+1).round(4)\n",
    "    del minute\n",
    "    day = 2*np.pi*((training_set['time']//1440)%365)/365\n",
    "    training_set['day'] = day\n",
    "    training_set['day_of_year_sin'] = (np.sin(day)+1).round(4)\n",
    "    training_set['day_of_year_cos'] = (np.cos(day)+1).round(4)\n",
    "    del day\n",
    "    weekday = 2*np.pi*((training_set['time']//1440)%7)/7\n",
    "    training_set['weekday'] = weekday\n",
    "    training_set['weekday_sin'] = (np.sin(weekday)+1).round(4)\n",
    "    training_set['weekday_cos'] = (np.cos(weekday)+1).round(4)\n",
    "    del weekday\n",
    "    training_set['year'] = (((training_set['time'])//525600))\n",
    "    training_set.drop(['time'], axis=1, inplace=True)\n",
    "    training_set['month'] = ((training_set['weekday']//30)%12+1)*2.73\n",
    "    training_set['accuracy'] = np.log10(training_set['accuracy'])*14.4\n",
    "\n",
    "#     training_set['accuracy'] = np.log10(training_set['accuracy'])\n",
    "    \n",
    "#     minute = training_set.time%60\n",
    "#     training_set['hour'] = training_set['time']//60\n",
    "#     training_set.drop(['time'], axis=1, inplace=True)\n",
    "#     training_set['weekday'] = training_set['hour']//24\n",
    "#     training_set['month'] = training_set['weekday']//30\n",
    "#     training_set['year'] = (training_set['weekday']//365+1)*0.22\n",
    "#     training_set['hour'] = ((training_set['hour']%24+1)+minute/60.0)*0.48\n",
    "#     del minute\n",
    "#     pd.options.mode.chained_assignment = None\n",
    "#     training_set['weekday'] = (training_set['weekday']%7+1)*1.55\n",
    "#     training_set['month'] = (training_set['month']%12+1)*2.73\n",
    "#     training_set['accuracy'] = np.log10(training_set['accuracy'])*14.4\n",
    "\n",
    "\n",
    "\n",
    "#     training_set.x_x.replace(0, .0001, inplace=True)\n",
    "#     training_set.y_x.replace(0, .0001, inplace=True)\n",
    "    training_set.loc[:,'x'] *= 465.0\n",
    "    training_set.loc[:,'y'] *= 975.0\n",
    "    training_set['squadd']= (training_set.x**2 + training_set.y**2)\n",
    "\n",
    "    \n",
    "    \n",
    "    minute = 2*np.pi*((testing_set[\"time\"]//5)%288)/288\n",
    "    testing_set['minute'] = minute\n",
    "    testing_set['minute_sin'] = (np.sin(minute)+1).round(4)\n",
    "    testing_set['minute_cos'] = (np.cos(minute)+1).round(4)\n",
    "    del minute\n",
    "    day = 2*np.pi*((testing_set['time']//1440)%365)/365\n",
    "    testing_set['day'] = day\n",
    "    testing_set['day_of_year_sin'] = (np.sin(day)+1).round(4)\n",
    "    testing_set['day_of_year_cos'] = (np.cos(day)+1).round(4)\n",
    "    del day\n",
    "    weekday = 2*np.pi*((testing_set['time']//1440)%7)/7\n",
    "    testing_set['weekday'] = weekday\n",
    "    testing_set['weekday_sin'] = (np.sin(weekday)+1).round(4)\n",
    "    testing_set['weekday_cos'] = (np.cos(weekday)+1).round(4)\n",
    "    del weekday\n",
    "    testing_set['year'] = (((testing_set['time'])//525600))\n",
    "    testing_set.drop(['time'], axis=1, inplace=True)\n",
    "    testing_set['month'] = ((testing_set['weekday']//30)%12+1)*2.73\n",
    "    testing_set['accuracy'] = np.log10(testing_set['accuracy'])*14.4\n",
    "\n",
    "#     testing_set['accuracy'] = np.log10(testing_set['accuracy'])\n",
    "    \n",
    "#     minute = testing_set['time']%60\n",
    "#     testing_set['hour'] = testing_set['time']//60\n",
    "#     testing_set.drop(['time'], axis=1, inplace=True)\n",
    "#     testing_set['weekday'] = testing_set['hour']//24\n",
    "#     testing_set['month'] = testing_set['weekday']//30\n",
    "#     testing_set['year'] = (testing_set['weekday']//365+1)*0.22\n",
    "#     testing_set['hour'] = ((testing_set['hour']%24+1)+minute/60.0)*0.48\n",
    "#     del minute\n",
    "#     testing_set['weekday'] = (testing_set['weekday']%7+1)*1.55\n",
    "#     testing_set['month'] = (testing_set['month']%12+1)*2.73\n",
    "#     testing_set['accuracy'] = np.log10(testing_set['accuracy'])*14.4\n",
    "\n",
    "\n",
    "#     testing_set.x_x.replace(0, .0001, inplace=True)\n",
    "#     testing_set.y_x.replace(0, .0001, inplace=True)\n",
    "    testing_set.loc[:,'x'] *= 465.0\n",
    "    testing_set.loc[:,'y'] *= 975.0\n",
    "    testing_set['squadd']= (testing_set.x**2 + testing_set.y**2)\n",
    "\n",
    "#     features = [c for c in training_set.columns if c in ['hour',\n",
    "#                                                          'weekday',\n",
    "#                                                          'month',\n",
    "#                                                          'year',\n",
    "#                                                          'hour',\n",
    "#                                                          'x', \n",
    "#                                                          'y',\n",
    "#                                                          'accuracy']]\n",
    "\n",
    "    features = [c for c in training_set.columns if c in ['minute_sin', 'minute_cos', 'day_of_year_sin', 'day_of_year_cos', 'weekday_sin', 'weekday_cos', 'year', 'accuracy', 'x', 'y']]\n",
    "\n",
    "    def calculate_distance(distances):\n",
    "        return distances ** -2\n",
    "\n",
    "    #Preparing data\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labels = le.fit_transform(training_set.place_id.values)\n",
    "\n",
    "#     neigh = neighbors.KNeighborsClassifier(n_neighbors=36, weights=calculate_distance, metric='manhattan', n_jobs=-1).fit(training_set.drop(['place_id', 'x_bin_100', 'y_bin_100'], axis=1).values, labels)\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(training_set.drop(['place_id', 'x_bin_100', 'y_bin_100'], axis=1).values, labels)\n",
    "#     bayes = naive_bayes.GaussianNB().fit(training_set[features], labels)\n",
    "#     boost = xgb.XGBClassifier(objective='multi:softprob', n_estimators=10, nthread=4).fit(training_set.drop(['place_id', 'x_bin_100', 'y_bin_100'], axis=1).values, labels)\n",
    "#     ensemb = EnsembleVoteClassifier(clfs=[forest, boost, bayes], weights=[2, 2, 1], voting='soft').fit(training_set[features], labels)\n",
    "    probs = forest.predict_proba(testing_set.drop(['place_id', 'x_bin_100', 'y_bin_100'], axis=1).values)\n",
    "#     probs.columns = le.inverse_transform(np.argsort(probs, axis=1)[:,::-1][:,:3])#np.unique(training_set['place_id'].sort_values().values)\n",
    "#     preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "    map3 = mapk([[l] for l in testing_set['place_id']], le.inverse_transform(np.argsort(probs, axis=1)[:,::-1][:,:3]), 3)\n",
    "    map3s.append(map3)\n",
    "    print \"Estimators:\", estimator, \"MapK:\", map3\n",
    "    print \"Done in:\",round(time()-t0,3),\"s\"\n",
    "print np.mean(map3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(training_set[features].shape[1]):\n",
    "    print(\"%d. feature %d - %s (%f)\" % (f + 1, indices[f], training_set[features].columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(training_set[features].shape[1]), importances[indices],\n",
    "       color=\"bisque\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(training_set[features].shape[1]), indices)\n",
    "plt.xlim([-1, training_set[features].shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train['seconds'] = (train['time'] * 60)\n",
    "train['date_time'] = pd.to_datetime(train['seconds'],unit='s')\n",
    "train['hour'] = train['date_time'].dt.hour\n",
    "train['day'] = train['date_time'].dt.day\n",
    "train['dow'] = train['date_time'].dt.dayofweek\n",
    "train.x.replace(0, .0001, inplace=True)\n",
    "train.y.replace(0, .0001, inplace=True)\n",
    "train['div']= (train.x / train.y)\n",
    "train['multi']= (train.x * train.y)\n",
    "train['squadd']= (train.x**2 + train.y**2)\n",
    "train['acc_squ'] = (train.accuracy**2 / (train.x / train.y))\n",
    "train['acc_x'] = (train.accuracy * train.x)\n",
    "train['acc_y'] = (train.accuracy * train.y)\n",
    "train['vector'] = ()\n",
    "\n",
    "# test['seconds'] = (test['time'] * 60)\n",
    "# test['date_time'] = pd.to_datetime(test['seconds'],unit='s')\n",
    "# test['hour'] = test['date_time'].dt.hour\n",
    "# test['day'] = test['date_time'].dt.day\n",
    "# test['dow'] = test['date_time'].dt.dayofweek\n",
    "# test.x.replace(0, .0001, inplace=True)\n",
    "# test.y.replace(0, .0001, inplace=True)\n",
    "# test['div'] = (test.x / test.y)\n",
    "# test['multi'] = (test.x * test.y)\n",
    "# test['squadd'] = (test.x**2 + test.y**2)\n",
    "# test['acc_squ'] = (test.accuracy**2 / (test.x / test.y))\n",
    "# test['acc_x'] = (test.accuracy * test.x)\n",
    "# test['acc_y'] = (test.accuracy * test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train, labels_train, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# estimators = [20,30,40,50,60,65,70]\n",
    "# for estimator in estimators :\n",
    "estimator = 60\n",
    "t0 = time()\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print \"Estimators:\", estimator, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test:  2.0 2.25 1.0 1.25 22684 5978\n",
    "# Test:  1.75 2.0 9.5 9.75 21100 6806\n",
    "train_reduced = train[(train.x >= 0) & (train.x < .1) & (train.y >= 0) & (train.y < .1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_reduced['day_number'] = ((train_reduced['time']/60)//24).astype(int)\n",
    "train_reduced['seconds'] = (train_reduced['time'] * 60)\n",
    "train_reduced['date_time'] = pd.to_datetime(train_reduced['seconds'],unit='s')\n",
    "train_reduced['hour'] = train_reduced['date_time'].dt.hour\n",
    "train_reduced['day'] = train_reduced['date_time'].dt.day\n",
    "train_reduced['dow'] = train_reduced['date_time'].dt.dayofweek\n",
    "train_reduced.x.replace(0, .0001, inplace=True)\n",
    "train_reduced.y.replace(0, .0001, inplace=True)\n",
    "train_reduced['div']= (train_reduced.x / train_reduced.y)\n",
    "train_reduced['multi']= (train_reduced.x * train_reduced.y)\n",
    "train_reduced['squadd']= (train_reduced.x**2 + train_reduced.y**2)\n",
    "train_reduced['acc_squ'] = (train_reduced.accuracy**2 / (train_reduced.x / train_reduced.y))\n",
    "train_reduced['acc_x'] = (train_reduced.accuracy * train_reduced.x)\n",
    "train_reduced['acc_y'] = (train_reduced.accuracy * train_reduced.y)\n",
    "train_reduced['vector'] = np.sqrt(train_reduced.x**2 + train_reduced.y**2 + train_reduced.hour**2 + train_reduced.dow**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_reduced.columns if c in ['x', 'y', 'accuracy', 'hour', 'day', 'dow', 'div', 'multi', 'squadd', 'acc_squ', 'acc_x', 'acc_y', 'vector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train, labels_train, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# estimators = [20,30,40,50,60,65,70]\n",
    "# for estimator in estimators :\n",
    "estimator = 60\n",
    "t0 = time()\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print \"Estimators:\", estimator, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reduced[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_leaf_options = [1,5,10,50,100,200,500]\n",
    "for leaf_size in sample_leaf_options:\n",
    "    t0 = time()\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators=60, min_samples_leaf=leaf_size, n_jobs=-1).fit(features_train, labels_train)\n",
    "    probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "    probs.columns = np.unique(labels_train.sort_values().values)\n",
    "    preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "    print \"Leafs:\", leaf_size, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "    print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test:  2.0 2.25 1.0 1.25 22684 5978\n",
    "# Test:  1.75 2.0 9.5 9.75 21100 6806\n",
    "# estimators = [20,30,40,50,60,65,70]\n",
    "for i in np.arange(4,5,.1):\n",
    "    train_reduced = train[(train.x >= 4) & (train.x <= 4.1) & (train.y >= i) & (train.y <= (i + .1))]\n",
    "    t0 = time()\n",
    "    train_reduced['seconds'] = (train_reduced['time'] * 60)\n",
    "    train_reduced['date_time'] = pd.to_datetime(train_reduced['seconds'],unit='s')\n",
    "    train_reduced['hour'] = train_reduced['date_time'].dt.hour\n",
    "    train_reduced['day'] = train_reduced['date_time'].dt.day\n",
    "    train_reduced['dow'] = train_reduced['date_time'].dt.dayofweek\n",
    "    train_reduced.x.replace(0, .0001, inplace=True)\n",
    "    train_reduced.y.replace(0, .0001, inplace=True)\n",
    "    train_reduced['div']= (train_reduced.x / train_reduced.y)\n",
    "    train_reduced['multi']= (train_reduced.x * train_reduced.y)\n",
    "    train_reduced['squadd']= (train_reduced.x**2 + train_reduced.y**2)\n",
    "    train_reduced['acc_squ'] = (train_reduced.accuracy**2 / (train_reduced.x / train_reduced.y))\n",
    "    train_reduced['acc_x'] = (train_reduced.accuracy * train_reduced.x)\n",
    "    train_reduced['acc_y'] = (train_reduced.accuracy * train_reduced.y)\n",
    "    features = [c for c in train_reduced.columns if c in ['x', 'y', 'accuracy', 'hour', 'day', 'dow', 'div', 'multi', 'squadd', 'acc_squ', 'acc_x', 'acc_y']]\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators=60, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "    probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "    probs.columns = np.unique(labels_train.sort_values().values)\n",
    "    preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "    print \"Estimators:\", 60, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "    print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# neigh = neighbors.KNeighborsClassifier(weights='distance', n_jobs=-1).fit(features_train, labels_train)\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=60, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "# bayes = naive_bayes.GaussianNB().fit(features_train.toarray(), labels_train)\n",
    "# boost = xgb.XGBClassifier(learning_rate=1,\n",
    "#                           n_estimators=10,\n",
    "#                           max_depth=5,\n",
    "#                           min_child_weight=1,\n",
    "#                           gamma=0,\n",
    "#                           subsample=0.8,\n",
    "#                           colsample_bytree=0.8,\n",
    "#                           nthread=4,\n",
    "#                           scale_pos_weight=1,\n",
    "#                           seed=27,\n",
    "#                           objective='multi:softprob').fit(features_train, labels_train)\n",
    "#  learning_rate = 1,\n",
    "#  max_depth=5,\n",
    "#  min_child_weight=1,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'multi:softprob',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=1,\n",
    "#  seed=27).fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_reduced['date_time'].min(), train_reduced['date_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = 60\n",
    "i_bin_x = 0\n",
    "i_bin_y = 0\n",
    "# for i_bin_y in bin_numbers:\n",
    "t0 = time()\n",
    "print(\"Bin {},{}\".format(i_bin_x, i_bin_y))\n",
    "#     print(\"Bin {}\".format( i_bin_y))\n",
    "\n",
    "# choose the correct bin, sort values in time to better simulate\n",
    "train_in_bin = train[(train.x_bin_100 == i_bin_x) & (train.y_bin_100 == i_bin_y)].sort_values('time')\n",
    "#     train_in_bin = train[(train.y_bin_1000 == i_bin_y)].sort_values('time')\n",
    "\n",
    "print 'Place IDs:', len(np.unique(train_in_bin['place_id']))\n",
    "\n",
    "training_set = train_in_bin[:int(len(train_in_bin) - (len(train_in_bin)*.25))]\n",
    "#     training_set = train_in_bin[int(len(train_in_bin)*.1):int(len(train_in_bin) - (len(train_in_bin)*.35))]\n",
    "\n",
    "testing_set = train_in_bin[int(len(train_in_bin) - (len(train_in_bin)*.25)):]\n",
    "\n",
    "training_set['day_number'] = ((training_set['time']/60)//24).astype(int)\n",
    "training_set['hour'] = (training_set['time']//60)%24+1 # 1 to 24\n",
    "training_set['dow'] = (training_set['time']//1440)%7+1\n",
    "training_set['month'] = (training_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "training_set['year'] = (training_set['time']//525600)+1\n",
    "# training_set['seconds'] = (training_set['time'] * 60)\n",
    "# training_set['date_time'] = pd.to_datetime(training_set['seconds'],unit='s')\n",
    "# training_set['hour'] = training_set['date_time'].dt.hour\n",
    "# training_set['dow'] = training_set['date_time'].dt.dayofweek\n",
    "# training_set['week_of_year'] = training_set['date_time'].dt.weekofyear\n",
    "\n",
    "\n",
    "accuracy_means = training_set.groupby(['place_id'], as_index=False)[[\"x\", \"y\", \"accuracy\"]].mean()\n",
    "time_mean = training_set.groupby(['place_id'], as_index=False)[[\"hour\", \"dow\", \"month\"]].mean()\n",
    "\n",
    "accuracy_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(accuracy_means[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "time_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(time_mean[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "\n",
    "training_set['acc_kde'] = accuracy_kde.score_samples(training_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "training_set['time_kde'] = time_kde.score_samples(training_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "mean_group = training_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "training_set = pd.merge(training_set, mean_group, on='hour')\n",
    "training_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(training_set.accuracy.astype(np.float)).reshape((len(training_set.accuracy), 1)))\n",
    "#     training_set['time_proportion'] = ((training_set.time - training_set.time.min()) / (training_set.time.max() - training_set.time.min()))\n",
    "\n",
    "#     training_set['r'] = np.sqrt((training_set.x_x-training_set.x_y)**2+(training_set.y_x-training_set.y_y)**2)\n",
    "#     training_set['minute'] = training_set['date_time'].dt.minute\n",
    "\n",
    "#     training_set['day'] = training_set['date_time'].dt.day\n",
    "training_set.x_x.replace(0, .0001, inplace=True)\n",
    "training_set.y_x.replace(0, .0001, inplace=True)\n",
    "training_set['squadd']= (training_set.x_x**2 + training_set.y_x**2)\n",
    "#     training_set['acc_squ'] = (training_set.accuracy**2 / (training_set.x / training_set.y))\n",
    "#     training_set['acc_x'] = (training_set.accuracy * training_set.x)\n",
    "#     training_set['acc_y'] = (training_set.accuracy * training_set.y)\n",
    "#     training_set['time_change'] = ((training_set.time - np.mean(training_set.time))/np.std(training_set.time))\n",
    "\n",
    "testing_set['hour'] = (testing_set['time']//60)%24+1 # 1 to 24\n",
    "testing_set['dow'] = (testing_set['time']//1440)%7+1\n",
    "testing_set['month'] = (testing_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "testing_set['year'] = (testing_set['time']//525600)+1\n",
    "\n",
    "# testing_set['seconds'] = (testing_set['time'] * 60)\n",
    "# testing_set['date_time'] = pd.to_datetime(testing_set['seconds'],unit='s')\n",
    "# testing_set['hour'] = testing_set['date_time'].dt.hour\n",
    "# testing_set['dow'] = testing_set['date_time'].dt.dayofweek\n",
    "# testing_set['week_of_year'] = testing_set['date_time'].dt.weekofyear\n",
    "\n",
    "testing_set['acc_kde'] = accuracy_kde.score_samples(testing_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "testing_set['time_kde'] = time_kde.score_samples(testing_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "mean_group = testing_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "testing_set = pd.merge(testing_set, mean_group, on='hour')\n",
    "testing_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(testing_set.accuracy.astype(np.float)).reshape((len(testing_set.accuracy), 1)))\n",
    "#     testing_set['time_proportion'] = (abs((testing_set.time - testing_set.time.min()) - testing_set.time.max()) / (testing_set.time.max() - testing_set.time.min()))\n",
    "\n",
    "#     testing_set['r'] = np.sqrt((testing_set.x_x-testing_set.x_y)**2+(testing_set.y_x-testing_set.y_y)**2)\n",
    "#     testing_set['minute'] = testing_set['date_time'].dt.minute\n",
    "\n",
    "testing_set.x_x.replace(0, .0001, inplace=True)\n",
    "testing_set.y_x.replace(0, .0001, inplace=True)\n",
    "testing_set['squadd']= (testing_set.x_x**2 + testing_set.y_x**2)\n",
    "#     testing_set['acc_squ'] = (testing_set.accuracy**2 / (testing_set.x / testing_set.y))\n",
    "#     testing_set['acc_x'] = (testing_set.accuracy * testing_set.x)\n",
    "#     testing_set['acc_y'] = (testing_set.accuracy * testing_set.y)\n",
    "#     testing_set['time_change'] = ((testing_set.time - np.mean(testing_set.time))/np.std(testing_set.time))\n",
    "\n",
    "features = [c for c in training_set.columns if c in ['year','month','dow','squadd','hour','time_kde','x_x', 'y_x','acc_norm']]\n",
    "\n",
    "\n",
    "#     features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_in_bin[features], train_in_bin['place_id'], test_size=0.70)\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(training_set[features], training_set['place_id'])\n",
    "#     boost = xgb.XGBClassifier(objective='multi:softprob', n_estimators=estimator, nthread=4).fit(training_set[features], training_set['place_id'])\n",
    "probs = pd.DataFrame(forest.predict_proba(testing_set[features]))\n",
    "probs.columns = np.unique(training_set['place_id'].sort_values().values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "map3 = mapk([[l] for l in testing_set['place_id']], preds[0], 3)\n",
    "# map3s.append(map3)\n",
    "print \"Estimators:\", estimator, \"MapK:\", map3\n",
    "print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training_set = train[(train.x > 5) & (train.x < 5.25) & (train.y > 5) & (train.y < 5.25)]\n",
    "training_set = train[train.x_bin_100 == 50]\n",
    "training_set['day_number'] = ((training_set['time']/60)//24).astype(int)\n",
    "training_set['hour'] = (training_set['time']//60)%24+1 # 1 to 24\n",
    "training_set['dow'] = (training_set['time']//1440)%7+1\n",
    "training_set['month'] = (training_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "training_set['year'] = (training_set['time']//525600)+1\n",
    "# training_set['seconds'] = (training_set['time'] * 60)\n",
    "# training_set['date_time'] = pd.to_datetime(training_set['seconds'],unit='s')\n",
    "# training_set['hour'] = training_set['date_time'].dt.hour\n",
    "# training_set['dow'] = training_set['date_time'].dt.dayofweek\n",
    "# training_set['week_of_year'] = training_set['date_time'].dt.weekofyear\n",
    "\n",
    "\n",
    "accuracy_means = training_set.groupby(['place_id'], as_index=False)[[\"x\", \"y\", \"accuracy\"]].mean()\n",
    "time_mean = training_set.groupby(['place_id'], as_index=False)[[\"hour\", \"dow\", \"month\"]].mean()\n",
    "\n",
    "accuracy_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(accuracy_means[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "time_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(time_mean[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "\n",
    "training_set['acc_kde'] = accuracy_kde.score_samples(training_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "training_set['time_kde'] = time_kde.score_samples(training_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "mean_group = training_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "training_set = pd.merge(training_set, mean_group, on='hour')\n",
    "training_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(training_set.accuracy.astype(np.float)).reshape((len(training_set.accuracy), 1)))\n",
    "#     training_set['time_proportion'] = ((training_set.time - training_set.time.min()) / (training_set.time.max() - training_set.time.min()))\n",
    "\n",
    "#     training_set['r'] = np.sqrt((training_set.x_x-training_set.x_y)**2+(training_set.y_x-training_set.y_y)**2)\n",
    "#     training_set['minute'] = training_set['date_time'].dt.minute\n",
    "\n",
    "#     training_set['day'] = training_set['date_time'].dt.day\n",
    "training_set.x_x.replace(0, .0001, inplace=True)\n",
    "training_set.y_x.replace(0, .0001, inplace=True)\n",
    "training_set['squadd']= (training_set.x_x**2 + training_set.y_x**2)\n",
    "#     training_set['acc_squ'] = (training_set.accuracy**2 / (training_set.x / training_set.y))\n",
    "#     training_set['acc_x'] = (training_set.accuracy * training_set.x)\n",
    "#     training_set['acc_y'] = (training_set.accuracy * training_set.y)\n",
    "#     training_set['time_change'] = ((training_set.time - np.mean(training_set.time))/np.std(training_set.time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.hist(training_set.day_number, bins=100, histtype = 'step')\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.hist(training_set.dow, bins=7)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.hist(training_set.hour, bins=24)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.scatter(training_set.x_x,training_set.y_x, c=training_set.place_id)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "small_counts = training_set['place_id'].value_counts()\n",
    "small_trainz = training_set[training_set['place_id'].isin(small_counts[small_counts > 100].index)]\n",
    "print len(small_trainz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "small_counts = training_set['place_id'].value_counts()\n",
    "small_trainz = training_set[training_set['place_id'].isin(small_counts[small_counts > 5].index)]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(small_trainz.x, small_trainz.y, zs=small_trainz.hour, zdir='z', s=20, c=small_trainz.place_id, depthshade=True)\n",
    "# ax.scatter(training_set.x_x, training_set.y_x, zs=training_set.hour, zdir='z', s=20, c=training_set.place_id, depthshade=True)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "labels = train_reduced['place_id']\n",
    "train_ids = train_reduced['row_id']\n",
    "train_reduced = train_reduced[features]\n",
    "# Transform the string labels to numerical using label encoder \n",
    "train_reduced.hour = le.fit_transform(train_reduced.hour)\n",
    "train_reduced.day = le.fit_transform(train_reduced.day)\n",
    "train_reduced.dow = le.fit_transform(train_reduced.dow)\n",
    "\n",
    "categ = [list(train_reduced.columns).index(x) for x in 'hour', 'dow', 'day']\n",
    "enc = preprocessing.OneHotEncoder(categorical_features = categ)\n",
    "train_reduced['hour'] = pd.factorize(train_reduced['hour'])[0]\n",
    "train_reduced['day'] = pd.factorize(train_reduced['day'])[0]\n",
    "train_reduced['dow'] = pd.factorize(train_reduced['dow'])[0]\n",
    "train_reduced = enc.fit_transform(train_reduced)\n",
    "print train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "labels = train_reduced['place_id']\n",
    "train_ids = train_reduced['row_id']\n",
    "train_reduced = train_reduced[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train, labels_train, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(boost.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(bayes.predict_proba(features_test.toarray()))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(neigh.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_check = pd.read_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_check.drop_duplicates('row_id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dups = submission_check[submission_check.duplicated(['row_id'])].sort_values('row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(test), len(submission_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
