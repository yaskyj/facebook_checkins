{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 713 ms, total: 2.12 s\n",
      "Wall time: 5.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ml_metrics as metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as itertools\n",
    "from sklearn import cross_validation, ensemble, tree, preprocessing, neighbors, naive_bayes, svm, cluster, linear_model\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "le = preprocessing.LabelEncoder()\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 2.32 s, total: 25.5 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def horizontally_bin_data(data, NX, NY):\n",
    "    \"\"\"Add columns to data indicating X and Y bins.\n",
    "\n",
    "    Divides the grid into `NX` bins in X and `NY` bins in Y, and adds columns \n",
    "    to `data` containing the bin number in X and Y. \n",
    "    \"\"\"\n",
    "\n",
    "    NX = int(NX)\n",
    "    NY = int(NY)\n",
    "\n",
    "    assert((NX >= 5) and (NX <= 1000))\n",
    "    assert((NY >= 5) and (NY <= 1000))\n",
    "\n",
    "    x_bounds = (0., 10.)\n",
    "    y_bounds = (0., 10.)\n",
    "\n",
    "    delta_X = (x_bounds[1] - x_bounds[0]) / float(NX)\n",
    "    delta_Y = (y_bounds[1] - y_bounds[0]) / float(NY)\n",
    "\n",
    "    # very fast binning algorithm, just divide by delta and round down\n",
    "    xbins = np.floor((data.x.values - x_bounds[0])\n",
    "                     / delta_X).astype(np.int32)\n",
    "    ybins = np.floor((data.y.values - y_bounds[0])\n",
    "                     / delta_Y).astype(np.int32)\n",
    "\n",
    "    # some points fall on the upper/right edge of the domain\n",
    "    # tweak their index to bring them back in the box\n",
    "    xbins[xbins == NX] = NX-1\n",
    "    ybins[ybins == NY] = NY-1\n",
    "\n",
    "    xlabel = 'x_bin_{0:03d}'.format(NX)\n",
    "    ylabel = 'y_bin_{0:03d}'.format(NY)\n",
    "\n",
    "    data[xlabel] = xbins\n",
    "    data[ylabel] = ybins\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 805 ms, sys: 594 ms, total: 1.4 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "horizontally_bin_data(train, 100, 100)\n",
    "# horizontally_bin_data(test, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 33,89\n",
      "Place IDs: 260\n",
      "Estimators: 60 MapK: 0.531746031746\n",
      "Done in: 3.547 s\n",
      "Bin 85,29\n",
      "Place IDs: 245\n",
      "Estimators: 60 MapK: 0.687827911857\n",
      "Done in: 4.83 s\n",
      "Bin 68,20\n",
      "Place IDs: 231\n",
      "Estimators: 60 MapK: 0.607442348008\n",
      "Done in: 4.561 s\n",
      "Bin 35,96\n",
      "Place IDs: 259\n",
      "Estimators: 60 MapK: 0.480482794042\n",
      "Done in: 3.618 s\n",
      "Bin 69,62\n",
      "Place IDs: 279\n",
      "Estimators: 60 MapK: 0.505312868949\n",
      "Done in: 5.397 s\n",
      "Bin 11,5\n",
      "Place IDs: 211\n",
      "Estimators: 60 MapK: 0.656592203241\n",
      "Done in: 3.638 s\n",
      "Bin 78,20\n",
      "Place IDs: 238\n",
      "Estimators: 60 MapK: 0.451264367816\n",
      "Done in: 3.598 s\n",
      "Bin 29,51\n",
      "Place IDs: 275\n",
      "Estimators: 60 MapK: 0.626666666667\n",
      "Done in: 3.06 s\n",
      "Bin 18,19\n",
      "Place IDs: 244\n",
      "Estimators: 60 MapK: 0.578158005915\n",
      "Done in: 3.939 s\n",
      "Bin 29,85\n",
      "Place IDs: 288\n",
      "Estimators: 60 MapK: 0.434607020814\n",
      "Done in: 5.72 s\n",
      "0.556010021906\n",
      "CPU times: user 43.9 s, sys: 2.62 s, total: 46.5 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = 60\n",
    "rs = np.random.RandomState(34)\n",
    "bin_numbers = zip(rs.randint(0, 100, size=10), rs.randint(0, 100, size=10))\n",
    "map3s = []\n",
    "#Choose this line for the whole dataset.\n",
    "# for i_bin_x, i_bin_y in itertools.product(xrange(50,51), xrange(50,60)):\n",
    "for i_bin_x, i_bin_y in bin_numbers:\n",
    "    # for i_bin_y in bin_numbers:\n",
    "    t0 = time()\n",
    "    print(\"Bin {},{}\".format(i_bin_x, i_bin_y))\n",
    "    #     print(\"Bin {}\".format( i_bin_y))\n",
    "\n",
    "    # choose the correct bin, sort values in time to better simulate\n",
    "    train_in_bin = train[(train.x_bin_100 == i_bin_x) & (train.y_bin_100 == i_bin_y)].sort_values('time')\n",
    "    #     train_in_bin = train[(train.y_bin_1000 == i_bin_y)].sort_values('time')\n",
    "\n",
    "    print 'Place IDs:', len(np.unique(train_in_bin['place_id']))\n",
    "\n",
    "    training_set = train_in_bin[:int(len(train_in_bin) - (len(train_in_bin)*.25))]\n",
    "    #     training_set = train_in_bin[int(len(train_in_bin)*.1):int(len(train_in_bin) - (len(train_in_bin)*.35))]\n",
    "\n",
    "    testing_set = train_in_bin[int(len(train_in_bin) - (len(train_in_bin)*.25)):]\n",
    "\n",
    "    training_set['day_number'] = ((training_set['time']/60)//24).astype(int)\n",
    "    training_set['hour'] = (training_set['time']//60)%24+1 # 1 to 24\n",
    "    training_set['dow'] = (training_set['time']//1440)%7+1\n",
    "    training_set['month'] = (training_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "    training_set['year'] = (training_set['time']//525600)+1\n",
    "    # training_set['seconds'] = (training_set['time'] * 60)\n",
    "    # training_set['date_time'] = pd.to_datetime(training_set['seconds'],unit='s')\n",
    "    # training_set['hour'] = training_set['date_time'].dt.hour\n",
    "    # training_set['dow'] = training_set['date_time'].dt.dayofweek\n",
    "    # training_set['week_of_year'] = training_set['date_time'].dt.weekofyear\n",
    "\n",
    "\n",
    "    accuracy_means = training_set.groupby(['place_id'], as_index=False)[[\"x\", \"y\", \"accuracy\"]].mean()\n",
    "    time_mean = training_set.groupby(['place_id'], as_index=False)[[\"hour\", \"dow\", \"month\"]].mean()\n",
    "\n",
    "    accuracy_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(accuracy_means[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "    time_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(time_mean[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "\n",
    "    training_set['acc_kde'] = accuracy_kde.score_samples(training_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "    training_set['time_kde'] = time_kde.score_samples(training_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "    mean_group = training_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "    training_set = pd.merge(training_set, mean_group, on='hour')\n",
    "    training_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(training_set.accuracy.astype(np.float)).reshape((len(training_set.accuracy), 1)))\n",
    "    #     training_set['time_proportion'] = ((training_set.time - training_set.time.min()) / (training_set.time.max() - training_set.time.min()))\n",
    "\n",
    "    #     training_set['r'] = np.sqrt((training_set.x_x-training_set.x_y)**2+(training_set.y_x-training_set.y_y)**2)\n",
    "    #     training_set['minute'] = training_set['date_time'].dt.minute\n",
    "\n",
    "    #     training_set['day'] = training_set['date_time'].dt.day\n",
    "    training_set.x_x.replace(0, .0001, inplace=True)\n",
    "    training_set.y_x.replace(0, .0001, inplace=True)\n",
    "    training_set['squadd']= (training_set.x_x**2 + training_set.y_x**2)\n",
    "    #     training_set['acc_squ'] = (training_set.accuracy**2 / (training_set.x / training_set.y))\n",
    "    #     training_set['acc_x'] = (training_set.accuracy * training_set.x)\n",
    "    #     training_set['acc_y'] = (training_set.accuracy * training_set.y)\n",
    "    #     training_set['time_change'] = ((training_set.time - np.mean(training_set.time))/np.std(training_set.time))\n",
    "\n",
    "    testing_set['hour'] = (testing_set['time']//60)%24+1 # 1 to 24\n",
    "    testing_set['dow'] = (testing_set['time']//1440)%7+1\n",
    "    testing_set['month'] = (testing_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "    testing_set['year'] = (testing_set['time']//525600)+1\n",
    "\n",
    "    # testing_set['seconds'] = (testing_set['time'] * 60)\n",
    "    # testing_set['date_time'] = pd.to_datetime(testing_set['seconds'],unit='s')\n",
    "    # testing_set['hour'] = testing_set['date_time'].dt.hour\n",
    "    # testing_set['dow'] = testing_set['date_time'].dt.dayofweek\n",
    "    # testing_set['week_of_year'] = testing_set['date_time'].dt.weekofyear\n",
    "\n",
    "    testing_set['acc_kde'] = accuracy_kde.score_samples(testing_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "    testing_set['time_kde'] = time_kde.score_samples(testing_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "    mean_group = testing_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "    testing_set = pd.merge(testing_set, mean_group, on='hour')\n",
    "    testing_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(testing_set.accuracy.astype(np.float)).reshape((len(testing_set.accuracy), 1)))\n",
    "    #     testing_set['time_proportion'] = (abs((testing_set.time - testing_set.time.min()) - testing_set.time.max()) / (testing_set.time.max() - testing_set.time.min()))\n",
    "\n",
    "    #     testing_set['r'] = np.sqrt((testing_set.x_x-testing_set.x_y)**2+(testing_set.y_x-testing_set.y_y)**2)\n",
    "    #     testing_set['minute'] = testing_set['date_time'].dt.minute\n",
    "\n",
    "    testing_set.x_x.replace(0, .0001, inplace=True)\n",
    "    testing_set.y_x.replace(0, .0001, inplace=True)\n",
    "    testing_set['squadd']= (testing_set.x_x**2 + testing_set.y_x**2)\n",
    "    #     testing_set['acc_squ'] = (testing_set.accuracy**2 / (testing_set.x / testing_set.y))\n",
    "    #     testing_set['acc_x'] = (testing_set.accuracy * testing_set.x)\n",
    "    #     testing_set['acc_y'] = (testing_set.accuracy * testing_set.y)\n",
    "    #     testing_set['time_change'] = ((testing_set.time - np.mean(testing_set.time))/np.std(testing_set.time))\n",
    "\n",
    "    features = [c for c in training_set.columns if c in ['year','month','dow','squadd','hour','time_kde','x_x', 'y_x','acc_norm']]\n",
    "\n",
    "\n",
    "    #     features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_in_bin[features], train_in_bin['place_id'], test_size=0.70)\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1)#.fit(training_set[features], training_set['place_id'])\n",
    "    bayes = naive_bayes.GaussianNB() #.fit(training_set[features], training_set['place_id'])\n",
    "    boost = xgb.XGBClassifier(objective='multi:softprob', n_estimators=10, nthread=4)#.fit(training_set[features], training_set['place_id'])\n",
    "    ensemb = EnsembleVoteClassifier(clfs=[forest, boost, bayes], weights=[2, 2, 1], voting='soft').fit(training_set[features], training_set['place_id'])\n",
    "    probs = pd.DataFrame(ensemb.predict_proba(testing_set[features]))\n",
    "    probs.columns = np.unique(training_set['place_id'].sort_values().values)\n",
    "    preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "    map3 = mapk([[l] for l in testing_set['place_id']], preds[0], 3)\n",
    "    map3s.append(map3)\n",
    "    print \"Estimators:\", estimator, \"MapK:\", map3\n",
    "    print \"Done in:\",round(time()-t0,3),\"s\"\n",
    "print np.mean(map3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call `fit` before `feature_importances_`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c461315a6b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n\u001b[1;32m      3\u001b[0m              axis=0)\n\u001b[1;32m      4\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arkham/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \"\"\"\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             raise NotFittedError(\"Estimator not fitted, \"\n\u001b[0m\u001b[1;32m    332\u001b[0m                                  \"call `fit` before `feature_importances_`.\")\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Estimator not fitted, call `fit` before `feature_importances_`."
     ]
    }
   ],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(training_set[features].shape[1]):\n",
    "    print(\"%d. feature %d - %s (%f)\" % (f + 1, indices[f], training_set[features].columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(training_set[features].shape[1]), importances[indices],\n",
    "       color=\"bisque\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(training_set[features].shape[1]), indices)\n",
    "plt.xlim([-1, training_set[features].shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train['seconds'] = (train['time'] * 60)\n",
    "train['date_time'] = pd.to_datetime(train['seconds'],unit='s')\n",
    "train['hour'] = train['date_time'].dt.hour\n",
    "train['day'] = train['date_time'].dt.day\n",
    "train['dow'] = train['date_time'].dt.dayofweek\n",
    "train.x.replace(0, .0001, inplace=True)\n",
    "train.y.replace(0, .0001, inplace=True)\n",
    "train['div']= (train.x / train.y)\n",
    "train['multi']= (train.x * train.y)\n",
    "train['squadd']= (train.x**2 + train.y**2)\n",
    "train['acc_squ'] = (train.accuracy**2 / (train.x / train.y))\n",
    "train['acc_x'] = (train.accuracy * train.x)\n",
    "train['acc_y'] = (train.accuracy * train.y)\n",
    "train['vector'] = ()\n",
    "\n",
    "# test['seconds'] = (test['time'] * 60)\n",
    "# test['date_time'] = pd.to_datetime(test['seconds'],unit='s')\n",
    "# test['hour'] = test['date_time'].dt.hour\n",
    "# test['day'] = test['date_time'].dt.day\n",
    "# test['dow'] = test['date_time'].dt.dayofweek\n",
    "# test.x.replace(0, .0001, inplace=True)\n",
    "# test.y.replace(0, .0001, inplace=True)\n",
    "# test['div'] = (test.x / test.y)\n",
    "# test['multi'] = (test.x * test.y)\n",
    "# test['squadd'] = (test.x**2 + test.y**2)\n",
    "# test['acc_squ'] = (test.accuracy**2 / (test.x / test.y))\n",
    "# test['acc_x'] = (test.accuracy * test.x)\n",
    "# test['acc_y'] = (test.accuracy * test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train, labels_train, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# estimators = [20,30,40,50,60,65,70]\n",
    "# for estimator in estimators :\n",
    "estimator = 60\n",
    "t0 = time()\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print \"Estimators:\", estimator, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test:  2.0 2.25 1.0 1.25 22684 5978\n",
    "# Test:  1.75 2.0 9.5 9.75 21100 6806\n",
    "train_reduced = train[(train.x >= 0) & (train.x < .1) & (train.y >= 0) & (train.y < .1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_reduced['day_number'] = ((train_reduced['time']/60)//24).astype(int)\n",
    "train_reduced['seconds'] = (train_reduced['time'] * 60)\n",
    "train_reduced['date_time'] = pd.to_datetime(train_reduced['seconds'],unit='s')\n",
    "train_reduced['hour'] = train_reduced['date_time'].dt.hour\n",
    "train_reduced['day'] = train_reduced['date_time'].dt.day\n",
    "train_reduced['dow'] = train_reduced['date_time'].dt.dayofweek\n",
    "train_reduced.x.replace(0, .0001, inplace=True)\n",
    "train_reduced.y.replace(0, .0001, inplace=True)\n",
    "train_reduced['div']= (train_reduced.x / train_reduced.y)\n",
    "train_reduced['multi']= (train_reduced.x * train_reduced.y)\n",
    "train_reduced['squadd']= (train_reduced.x**2 + train_reduced.y**2)\n",
    "train_reduced['acc_squ'] = (train_reduced.accuracy**2 / (train_reduced.x / train_reduced.y))\n",
    "train_reduced['acc_x'] = (train_reduced.accuracy * train_reduced.x)\n",
    "train_reduced['acc_y'] = (train_reduced.accuracy * train_reduced.y)\n",
    "train_reduced['vector'] = np.sqrt(train_reduced.x**2 + train_reduced.y**2 + train_reduced.hour**2 + train_reduced.dow**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_reduced.columns if c in ['x', 'y', 'accuracy', 'hour', 'day', 'dow', 'div', 'multi', 'squadd', 'acc_squ', 'acc_x', 'acc_y', 'vector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train, labels_train, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# estimators = [20,30,40,50,60,65,70]\n",
    "# for estimator in estimators :\n",
    "estimator = 60\n",
    "t0 = time()\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print \"Estimators:\", estimator, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reduced[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_leaf_options = [1,5,10,50,100,200,500]\n",
    "for leaf_size in sample_leaf_options:\n",
    "    t0 = time()\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators=60, min_samples_leaf=leaf_size, n_jobs=-1).fit(features_train, labels_train)\n",
    "    probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "    probs.columns = np.unique(labels_train.sort_values().values)\n",
    "    preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "    print \"Leafs:\", leaf_size, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "    print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test:  2.0 2.25 1.0 1.25 22684 5978\n",
    "# Test:  1.75 2.0 9.5 9.75 21100 6806\n",
    "# estimators = [20,30,40,50,60,65,70]\n",
    "for i in np.arange(4,5,.1):\n",
    "    train_reduced = train[(train.x >= 4) & (train.x <= 4.1) & (train.y >= i) & (train.y <= (i + .1))]\n",
    "    t0 = time()\n",
    "    train_reduced['seconds'] = (train_reduced['time'] * 60)\n",
    "    train_reduced['date_time'] = pd.to_datetime(train_reduced['seconds'],unit='s')\n",
    "    train_reduced['hour'] = train_reduced['date_time'].dt.hour\n",
    "    train_reduced['day'] = train_reduced['date_time'].dt.day\n",
    "    train_reduced['dow'] = train_reduced['date_time'].dt.dayofweek\n",
    "    train_reduced.x.replace(0, .0001, inplace=True)\n",
    "    train_reduced.y.replace(0, .0001, inplace=True)\n",
    "    train_reduced['div']= (train_reduced.x / train_reduced.y)\n",
    "    train_reduced['multi']= (train_reduced.x * train_reduced.y)\n",
    "    train_reduced['squadd']= (train_reduced.x**2 + train_reduced.y**2)\n",
    "    train_reduced['acc_squ'] = (train_reduced.accuracy**2 / (train_reduced.x / train_reduced.y))\n",
    "    train_reduced['acc_x'] = (train_reduced.accuracy * train_reduced.x)\n",
    "    train_reduced['acc_y'] = (train_reduced.accuracy * train_reduced.y)\n",
    "    features = [c for c in train_reduced.columns if c in ['x', 'y', 'accuracy', 'hour', 'day', 'dow', 'div', 'multi', 'squadd', 'acc_squ', 'acc_x', 'acc_y']]\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "    forest = ensemble.RandomForestClassifier(n_estimators=60, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "    probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "    probs.columns = np.unique(labels_train.sort_values().values)\n",
    "    preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "    print \"Estimators:\", 60, \"MapK:\", mapk([[l] for l in labels_test], preds[0], 3)\n",
    "    print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# neigh = neighbors.KNeighborsClassifier(weights='distance', n_jobs=-1).fit(features_train, labels_train)\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=60, min_samples_leaf=5, n_jobs=-1).fit(features_train, labels_train)\n",
    "# bayes = naive_bayes.GaussianNB().fit(features_train.toarray(), labels_train)\n",
    "# boost = xgb.XGBClassifier(learning_rate=1,\n",
    "#                           n_estimators=10,\n",
    "#                           max_depth=5,\n",
    "#                           min_child_weight=1,\n",
    "#                           gamma=0,\n",
    "#                           subsample=0.8,\n",
    "#                           colsample_bytree=0.8,\n",
    "#                           nthread=4,\n",
    "#                           scale_pos_weight=1,\n",
    "#                           seed=27,\n",
    "#                           objective='multi:softprob').fit(features_train, labels_train)\n",
    "#  learning_rate = 1,\n",
    "#  max_depth=5,\n",
    "#  min_child_weight=1,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'multi:softprob',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=1,\n",
    "#  seed=27).fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_reduced['date_time'].min(), train_reduced['date_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = 60\n",
    "i_bin_x = 0\n",
    "i_bin_y = 0\n",
    "# for i_bin_y in bin_numbers:\n",
    "t0 = time()\n",
    "print(\"Bin {},{}\".format(i_bin_x, i_bin_y))\n",
    "#     print(\"Bin {}\".format( i_bin_y))\n",
    "\n",
    "# choose the correct bin, sort values in time to better simulate\n",
    "train_in_bin = train[(train.x_bin_100 == i_bin_x) & (train.y_bin_100 == i_bin_y)].sort_values('time')\n",
    "#     train_in_bin = train[(train.y_bin_1000 == i_bin_y)].sort_values('time')\n",
    "\n",
    "print 'Place IDs:', len(np.unique(train_in_bin['place_id']))\n",
    "\n",
    "training_set = train_in_bin[:int(len(train_in_bin) - (len(train_in_bin)*.25))]\n",
    "#     training_set = train_in_bin[int(len(train_in_bin)*.1):int(len(train_in_bin) - (len(train_in_bin)*.35))]\n",
    "\n",
    "testing_set = train_in_bin[int(len(train_in_bin) - (len(train_in_bin)*.25)):]\n",
    "\n",
    "training_set['day_number'] = ((training_set['time']/60)//24).astype(int)\n",
    "training_set['hour'] = (training_set['time']//60)%24+1 # 1 to 24\n",
    "training_set['dow'] = (training_set['time']//1440)%7+1\n",
    "training_set['month'] = (training_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "training_set['year'] = (training_set['time']//525600)+1\n",
    "# training_set['seconds'] = (training_set['time'] * 60)\n",
    "# training_set['date_time'] = pd.to_datetime(training_set['seconds'],unit='s')\n",
    "# training_set['hour'] = training_set['date_time'].dt.hour\n",
    "# training_set['dow'] = training_set['date_time'].dt.dayofweek\n",
    "# training_set['week_of_year'] = training_set['date_time'].dt.weekofyear\n",
    "\n",
    "\n",
    "accuracy_means = training_set.groupby(['place_id'], as_index=False)[[\"x\", \"y\", \"accuracy\"]].mean()\n",
    "time_mean = training_set.groupby(['place_id'], as_index=False)[[\"hour\", \"dow\", \"month\"]].mean()\n",
    "\n",
    "accuracy_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(accuracy_means[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "time_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(time_mean[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "\n",
    "training_set['acc_kde'] = accuracy_kde.score_samples(training_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "training_set['time_kde'] = time_kde.score_samples(training_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "mean_group = training_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "training_set = pd.merge(training_set, mean_group, on='hour')\n",
    "training_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(training_set.accuracy.astype(np.float)).reshape((len(training_set.accuracy), 1)))\n",
    "#     training_set['time_proportion'] = ((training_set.time - training_set.time.min()) / (training_set.time.max() - training_set.time.min()))\n",
    "\n",
    "#     training_set['r'] = np.sqrt((training_set.x_x-training_set.x_y)**2+(training_set.y_x-training_set.y_y)**2)\n",
    "#     training_set['minute'] = training_set['date_time'].dt.minute\n",
    "\n",
    "#     training_set['day'] = training_set['date_time'].dt.day\n",
    "training_set.x_x.replace(0, .0001, inplace=True)\n",
    "training_set.y_x.replace(0, .0001, inplace=True)\n",
    "training_set['squadd']= (training_set.x_x**2 + training_set.y_x**2)\n",
    "#     training_set['acc_squ'] = (training_set.accuracy**2 / (training_set.x / training_set.y))\n",
    "#     training_set['acc_x'] = (training_set.accuracy * training_set.x)\n",
    "#     training_set['acc_y'] = (training_set.accuracy * training_set.y)\n",
    "#     training_set['time_change'] = ((training_set.time - np.mean(training_set.time))/np.std(training_set.time))\n",
    "\n",
    "testing_set['hour'] = (testing_set['time']//60)%24+1 # 1 to 24\n",
    "testing_set['dow'] = (testing_set['time']//1440)%7+1\n",
    "testing_set['month'] = (testing_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "testing_set['year'] = (testing_set['time']//525600)+1\n",
    "\n",
    "# testing_set['seconds'] = (testing_set['time'] * 60)\n",
    "# testing_set['date_time'] = pd.to_datetime(testing_set['seconds'],unit='s')\n",
    "# testing_set['hour'] = testing_set['date_time'].dt.hour\n",
    "# testing_set['dow'] = testing_set['date_time'].dt.dayofweek\n",
    "# testing_set['week_of_year'] = testing_set['date_time'].dt.weekofyear\n",
    "\n",
    "testing_set['acc_kde'] = accuracy_kde.score_samples(testing_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "testing_set['time_kde'] = time_kde.score_samples(testing_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "mean_group = testing_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "testing_set = pd.merge(testing_set, mean_group, on='hour')\n",
    "testing_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(testing_set.accuracy.astype(np.float)).reshape((len(testing_set.accuracy), 1)))\n",
    "#     testing_set['time_proportion'] = (abs((testing_set.time - testing_set.time.min()) - testing_set.time.max()) / (testing_set.time.max() - testing_set.time.min()))\n",
    "\n",
    "#     testing_set['r'] = np.sqrt((testing_set.x_x-testing_set.x_y)**2+(testing_set.y_x-testing_set.y_y)**2)\n",
    "#     testing_set['minute'] = testing_set['date_time'].dt.minute\n",
    "\n",
    "testing_set.x_x.replace(0, .0001, inplace=True)\n",
    "testing_set.y_x.replace(0, .0001, inplace=True)\n",
    "testing_set['squadd']= (testing_set.x_x**2 + testing_set.y_x**2)\n",
    "#     testing_set['acc_squ'] = (testing_set.accuracy**2 / (testing_set.x / testing_set.y))\n",
    "#     testing_set['acc_x'] = (testing_set.accuracy * testing_set.x)\n",
    "#     testing_set['acc_y'] = (testing_set.accuracy * testing_set.y)\n",
    "#     testing_set['time_change'] = ((testing_set.time - np.mean(testing_set.time))/np.std(testing_set.time))\n",
    "\n",
    "features = [c for c in training_set.columns if c in ['year','month','dow','squadd','hour','time_kde','x_x', 'y_x','acc_norm']]\n",
    "\n",
    "\n",
    "#     features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_in_bin[features], train_in_bin['place_id'], test_size=0.70)\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=estimator, min_samples_leaf=5, n_jobs=-1).fit(training_set[features], training_set['place_id'])\n",
    "#     boost = xgb.XGBClassifier(objective='multi:softprob', n_estimators=estimator, nthread=4).fit(training_set[features], training_set['place_id'])\n",
    "probs = pd.DataFrame(forest.predict_proba(testing_set[features]))\n",
    "probs.columns = np.unique(training_set['place_id'].sort_values().values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "map3 = mapk([[l] for l in testing_set['place_id']], preds[0], 3)\n",
    "# map3s.append(map3)\n",
    "print \"Estimators:\", estimator, \"MapK:\", map3\n",
    "print \"Done in:\",round(time()-t0,3),\"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training_set = train[(train.x > 5) & (train.x < 5.25) & (train.y > 5) & (train.y < 5.25)]\n",
    "training_set = train[train.x_bin_100 == 50]\n",
    "training_set['day_number'] = ((training_set['time']/60)//24).astype(int)\n",
    "training_set['hour'] = (training_set['time']//60)%24+1 # 1 to 24\n",
    "training_set['dow'] = (training_set['time']//1440)%7+1\n",
    "training_set['month'] = (training_set['time']//43200)%12+1 # rough estimate, month = 30 days\n",
    "training_set['year'] = (training_set['time']//525600)+1\n",
    "# training_set['seconds'] = (training_set['time'] * 60)\n",
    "# training_set['date_time'] = pd.to_datetime(training_set['seconds'],unit='s')\n",
    "# training_set['hour'] = training_set['date_time'].dt.hour\n",
    "# training_set['dow'] = training_set['date_time'].dt.dayofweek\n",
    "# training_set['week_of_year'] = training_set['date_time'].dt.weekofyear\n",
    "\n",
    "\n",
    "accuracy_means = training_set.groupby(['place_id'], as_index=False)[[\"x\", \"y\", \"accuracy\"]].mean()\n",
    "time_mean = training_set.groupby(['place_id'], as_index=False)[[\"hour\", \"dow\", \"month\"]].mean()\n",
    "\n",
    "accuracy_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(accuracy_means[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "time_kde = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.2).fit(time_mean[[\"hour\", \"dow\", \"month\"]].values)\n",
    "\n",
    "\n",
    "training_set['acc_kde'] = accuracy_kde.score_samples(training_set[[\"x\", \"y\", \"accuracy\"]].values)\n",
    "training_set['time_kde'] = time_kde.score_samples(training_set[[\"hour\", \"dow\", \"month\"]].values)\n",
    "mean_group = training_set.groupby(['hour'], as_index=False)[[\"x\", \"y\"]].mean()\n",
    "training_set = pd.merge(training_set, mean_group, on='hour')\n",
    "training_set['acc_norm'] = preprocessing.MinMaxScaler(feature_range=(0,25)).fit_transform(np.array(training_set.accuracy.astype(np.float)).reshape((len(training_set.accuracy), 1)))\n",
    "#     training_set['time_proportion'] = ((training_set.time - training_set.time.min()) / (training_set.time.max() - training_set.time.min()))\n",
    "\n",
    "#     training_set['r'] = np.sqrt((training_set.x_x-training_set.x_y)**2+(training_set.y_x-training_set.y_y)**2)\n",
    "#     training_set['minute'] = training_set['date_time'].dt.minute\n",
    "\n",
    "#     training_set['day'] = training_set['date_time'].dt.day\n",
    "training_set.x_x.replace(0, .0001, inplace=True)\n",
    "training_set.y_x.replace(0, .0001, inplace=True)\n",
    "training_set['squadd']= (training_set.x_x**2 + training_set.y_x**2)\n",
    "#     training_set['acc_squ'] = (training_set.accuracy**2 / (training_set.x / training_set.y))\n",
    "#     training_set['acc_x'] = (training_set.accuracy * training_set.x)\n",
    "#     training_set['acc_y'] = (training_set.accuracy * training_set.y)\n",
    "#     training_set['time_change'] = ((training_set.time - np.mean(training_set.time))/np.std(training_set.time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.hist(training_set.day_number, bins=100, histtype = 'step')\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.hist(training_set.dow, bins=7)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.hist(training_set.hour, bins=24)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "plt.scatter(training_set.x_x,training_set.y_x, c=training_set.place_id)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "small_counts = training_set['place_id'].value_counts()\n",
    "small_trainz = training_set[training_set['place_id'].isin(small_counts[small_counts > 100].index)]\n",
    "print len(small_trainz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "small_counts = training_set['place_id'].value_counts()\n",
    "small_trainz = training_set[training_set['place_id'].isin(small_counts[small_counts > 800].index)]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(small_trainz.x_x, small_trainz.y_x, zs=small_trainz.hour, zdir='z', s=20, c=small_trainz.place_id, depthshade=True)\n",
    "# ax.scatter(training_set.x_x, training_set.y_x, zs=training_set.hour, zdir='z', s=20, c=training_set.place_id, depthshade=True)\n",
    "plt.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "labels = train_reduced['place_id']\n",
    "train_ids = train_reduced['row_id']\n",
    "train_reduced = train_reduced[features]\n",
    "# Transform the string labels to numerical using label encoder \n",
    "train_reduced.hour = le.fit_transform(train_reduced.hour)\n",
    "train_reduced.day = le.fit_transform(train_reduced.day)\n",
    "train_reduced.dow = le.fit_transform(train_reduced.dow)\n",
    "\n",
    "categ = [list(train_reduced.columns).index(x) for x in 'hour', 'dow', 'day']\n",
    "enc = preprocessing.OneHotEncoder(categorical_features = categ)\n",
    "train_reduced['hour'] = pd.factorize(train_reduced['hour'])[0]\n",
    "train_reduced['day'] = pd.factorize(train_reduced['day'])[0]\n",
    "train_reduced['dow'] = pd.factorize(train_reduced['dow'])[0]\n",
    "train_reduced = enc.fit_transform(train_reduced)\n",
    "print train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "labels = train_reduced['place_id']\n",
    "train_ids = train_reduced['row_id']\n",
    "train_reduced = train_reduced[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(train_reduced[features], train_reduced['place_id'], test_size=0.70)\n",
    "# features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train, labels_train, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(boost.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(forest.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(bayes.predict_proba(features_test.toarray()))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = pd.DataFrame(neigh.predict_proba(features_test))\n",
    "probs.columns = np.unique(labels_train.sort_values().values)\n",
    "#probs.columns = np.unique(labels_train.values)\n",
    "preds = pd.DataFrame([list([r.sort_values(ascending=False)[:3].index.values]) for i,r in probs.iterrows()])\n",
    "print mapk([[l] for l in labels_test], preds[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_check = pd.read_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_check.drop_duplicates('row_id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dups = submission_check[submission_check.duplicated(['row_id'])].sort_values('row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(test), len(submission_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
